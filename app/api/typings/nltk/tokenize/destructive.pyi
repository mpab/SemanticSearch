"""
This type stub file was generated by pyright.
"""

from nltk.tokenize.api import TokenizerI

class MacIntyreContractions:
    """
    List of contractions adapted from Robert MacIntyre's tokenizer.
    """
    CONTRACTIONS2 = ...
    CONTRACTIONS3 = ...
    CONTRACTIONS4 = ...


class NLTKWordTokenizer(TokenizerI):
    """
    The NLTK tokenizer that has improved upon the TreebankWordTokenizer.

    The tokenizer is "destructive" such that the regexes applied will munge the
    input string to a state beyond re-construction. It is possible to apply
    `TreebankWordDetokenizer.detokenize` to the tokenized outputs of
    `NLTKDestructiveWordTokenizer.tokenize` but there's no guarantees to
    revert to the original string.
    """
    STARTING_QUOTES = ...
    ENDING_QUOTES = ...
    PUNCTUATION = ...
    PARENS_BRACKETS = ...
    CONVERT_PARENTHESES = ...
    DOUBLE_DASHES = ...
    _contractions = ...
    CONTRACTIONS2 = ...
    CONTRACTIONS3 = ...
    def tokenize(self, text, convert_parentheses=..., return_str=...):
        ...
    


